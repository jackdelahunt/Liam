enum TokenType {
    NUMBER_LITERAL,
    STRING_LITERAL,
    IDENTIFIER,   
    LET, 
    INSERT,
    FN,   
    PAREN_OPEN, 
    PAREN_CLOSE, 
    BRACE_OPEN, 
    BRACE_CLOSE, 
    PLUS, 
    MINUS,
    STAR,
    SLASH, 
    MOD, 
    ASSIGN, 
    SEMI_COLON, 
    COMMA, 
    COLON, 
    RETURN, 
    HAT, 
    AT, 
    STRUCT, 
    DOT, 
    NEW, 
    BREAK, 
    IMPORT, 
    BRACKET_OPEN, 
    BRACKET_CLOSE, 
    FOR, 
    FALSE, 
    TRUE, 
    IF,
    ELSE, 
    OR,  
    AND,
    EQUAL, 
    NOT_EQUAL, 
    NOT, 
    LESS, 
    GREATER, 
    GREATER_EQUAL,
    LESS_EQUAL, 
    EXTERN, 
    BAR, 
    IS, 
    NULL_POINTER, 
    ENUM,
    CONTINUE,
    ALIAS,
    AS
}

struct Token {
    type: TokenType,
    string: str
}

struct Lexer {
    path: str,
    source: String,
    current: u64,
    tokens: Array[Token]
}

fn make_lexer(path: str): Lexer {
   return new Lexer {
        path: path,
        source: read(path),
        current: 0u64,
        tokens: make_array[Token]()
   }; 
}

fn next_char(lexer: ^Lexer): void {
    lexer.current = lexer.current + 1u64;
}

fn peek(lexer: ^Lexer): str {
    return substr(lexer.source.string, lexer.current + 1u64, 1u64);
}

fn lex(lexer: ^Lexer): void {
   for lexer.current = 0u64; lexer.current < len(lexer.source.string); next_char(lexer); {
        let current_char := substr(lexer.source.string, lexer.current, 1u64);
        
        if current_char == "\n" or current_char == " " or current_char == "\r" or current_char == "\t" {
           continue; 
        }

        if current_char == "+" {
            array_append[Token](@lexer.tokens, new Token{
                type: TokenType.PLUS,
                string: "+"
            });
            continue;
        }

        if current_char == "-" {
            array_append[Token](@lexer.tokens, new Token{
                type: TokenType.MINUS,
                string: "-"
            });
            continue;
        }

        if current_char == "*" {
            array_append[Token](@lexer.tokens, new Token{
                type: TokenType.STAR,
                string: "*"
            });
            continue;
        }

        if current_char == "/" {
            array_append[Token](@lexer.tokens, new Token{
                type: TokenType.SLASH,
                string: "/"
            });
            continue;
        }

        if current_char == "%" {
            array_append[Token](@lexer.tokens, new Token{
                type: TokenType.MOD,
                string: "%"
            });
            continue;
        }

        if current_char == "=" {
            if peek(lexer) == "=" {
                next_char(lexer);
                array_append[Token](@lexer.tokens, new Token{
                    type: TokenType.EQUAL,
                    string: "=="
                });
                continue;
            }

            array_append[Token](@lexer.tokens, new Token{
                type: TokenType.ASSIGN,
                string: "="
            });
            continue;
        }

        if current_char == ";" {
            array_append[Token](@lexer.tokens, new Token{
                type: TokenType.SEMI_COLON,
                string: ";"
            });
            continue;
        }

        if current_char == "(" {
            array_append[Token](@lexer.tokens, new Token{
                type: TokenType.PAREN_OPEN,
                string: "("
            });
            continue;
        }

        if current_char == ")" {
            array_append[Token](@lexer.tokens, new Token{
                type: TokenType.PAREN_CLOSE,
                string: ")"
            });
            continue;
        }

        if current_char == "{" {
            array_append[Token](@lexer.tokens, new Token{
                type: TokenType.BRACE_OPEN,
                string: "{"
            });
            continue;
        }

        if current_char == "}" {
            array_append[Token](@lexer.tokens, new Token{
                type: TokenType.BRACE_CLOSE,
                string: "}"
            });
            continue;
        }

        if current_char == "," {
            array_append[Token](@lexer.tokens, new Token{
                type: TokenType.COMMA,
                string: ","
            });
            continue;
        }

        if current_char == "[" {
            array_append[Token](@lexer.tokens, new Token{
                type: TokenType.BRACKET_OPEN,
                string: "["
            });
            continue;
        }

        if current_char == "]" {
            array_append[Token](@lexer.tokens, new Token{
                type: TokenType.BRACKET_CLOSE,
                string: "]"
            });
            continue;
        }

        if current_char == ":" {
            array_append[Token](@lexer.tokens, new Token{
                type: TokenType.COLON,
                string: ":"
            });
            continue;
        }

        if current_char == "^" {
            array_append[Token](@lexer.tokens, new Token{
                type: TokenType.HAT,
                string: "^"
            });
            continue;
        }

        if current_char == "@" {
            array_append[Token](@lexer.tokens, new Token{
                type: TokenType.AT,
                string: "@"
            });
            continue;
        }

        if current_char == "." {
            array_append[Token](@lexer.tokens, new Token{
                type: TokenType.DOT,
                string: "."
            });
            continue;
        }

        if current_char == "<" {

            if peek(lexer) == "=" {
                next_char(lexer);
                array_append[Token](@lexer.tokens, new Token{
                    type: TokenType.LESS_EQUAL,
                    string: "<="
                });
                continue;
            }

            array_append[Token](@lexer.tokens, new Token{
                type: TokenType.LESS,
                string: "<"
            });
            continue;
        }

        if current_char == ">" {

            if peek(lexer) == "=" {
                next_char(lexer);
                array_append[Token](@lexer.tokens, new Token{
                    type: TokenType.GREATER_EQUAL,
                    string: ">="
                });
                continue;
            }

            array_append[Token](@lexer.tokens, new Token{
                type: TokenType.GREATER,
                string: ">"
            });
            continue;
        }

        if current_char == "|" {
            array_append[Token](@lexer.tokens, new Token{
                type: TokenType.BAR,
                string: "|"
            });
            continue;
        }

        if current_char == "!" {

            if peek(lexer) == "=" {
                next_char(lexer);
                array_append[Token](@lexer.tokens, new Token{
                    type: TokenType.NOT_EQUAL,
                    string: "!="
                });
                continue;
            }

            array_append[Token](@lexer.tokens, new Token{
                type: TokenType.NOT,
                string: "!"
            });
            continue;
        }

        if current_char == "#" {
            for 
                let c := current_char; 
                lexer.current < len(lexer.source.string) and substr(lexer.source.string, lexer.current, 1u64) != "\n"; 
                next_char(lexer); 
            {}
            continue;
        }

        if current_char == "\"" {
            next_char(lexer); # starting quote
            let start := lexer.current;
            
            for 
                let c := 0; 
                lexer.current < len(lexer.source.string) and substr(lexer.source.string, lexer.current, 1u64) != "\""; 
                next_char(lexer); 
            {}

            array_append[Token](@lexer.tokens, new Token{
                type: TokenType.STRING_LITERAL,
                string: substr(lexer.source.string, start, lexer.current - start)
            });

            continue;
        }

        let word := get_word(lexer);

        if word == "let" {
            array_append[Token](@lexer.tokens, new Token{
                type: TokenType.LET,
                string: word
            });

            continue;
        }

        if word == "insert" {
            array_append[Token](@lexer.tokens, new Token{
                type: TokenType.INSERT,
                string: word
            });

            continue;
        }

        if word == "fn" {
            array_append[Token](@lexer.tokens, new Token{
                type: TokenType.FN,
                string: word
            });

            continue;
        }

        if word == "return" {
            array_append[Token](@lexer.tokens, new Token{
                type: TokenType.RETURN,
                string: word
            });

            continue;
        }

        if word == "struct" {
            array_append[Token](@lexer.tokens, new Token{
                type: TokenType.STRUCT,
                string: word
            });

            continue;
        }

        if word == "new" {
            array_append[Token](@lexer.tokens, new Token{
                type: TokenType.NEW,
                string: word
            });

            continue;
        }

        if word == "continue" {
            array_append[Token](@lexer.tokens, new Token{
                type: TokenType.CONTINUE,
                string: word
            });

            continue;
        }

        if word == "import" {
            array_append[Token](@lexer.tokens, new Token{
                type: TokenType.IMPORT,
                string: word
            });

            continue;
        }

        if word == "for" {
            array_append[Token](@lexer.tokens, new Token{
                type: TokenType.FOR,
                string: word
            });

            continue;
        }

        if word == "if" {
            array_append[Token](@lexer.tokens, new Token{
                type: TokenType.IF,
                string: word
            });

            continue;
        }

        if word == "else" {
            array_append[Token](@lexer.tokens, new Token{
                type: TokenType.ELSE,
                string: word
            });

            continue;
        }

        if word == "and" {
            array_append[Token](@lexer.tokens, new Token{
                type: TokenType.AND,
                string: word
            });

            continue;
        }

        if word == "or" {
            array_append[Token](@lexer.tokens, new Token{
                type: TokenType.OR,
                string: word
            });

            continue;
        }

        if word == "extern" {
            array_append[Token](@lexer.tokens, new Token{
                type: TokenType.EXTERN,
                string: word
            });

            continue;
        }

        if word == "is" {
            array_append[Token](@lexer.tokens, new Token{
                type: TokenType.IS,
                string: word
            });

            continue;
        }

        if word == "enum" {
            array_append[Token](@lexer.tokens, new Token{
                type: TokenType.ENUM,
                string: word
            });

            continue;
        }

        if word == "true" {
            array_append[Token](@lexer.tokens, new Token{
                type: TokenType.TRUE,
                string: word
            });

            continue;
        }

        if word == "false" {
            array_append[Token](@lexer.tokens, new Token{
                type: TokenType.FALSE,
                string: word
            });

            continue;
        }

        if word == "null" {
            array_append[Token](@lexer.tokens, new Token{
                type: TokenType.NULL_POINTER,
                string: word
            });

            continue;
        }

        if word == "alias" {
            array_append[Token](@lexer.tokens, new Token{
                type: TokenType.ALIAS,
                string: word
            });

            continue;
        }

        if word == "as" {
            array_append[Token](@lexer.tokens, new Token{
                type: TokenType.AS,
                string: word
            });

            continue;
        }

        array_append[Token](@lexer.tokens, new Token{
            type: TokenType.IDENTIFIER,
            string: word
        });
   }  
}

fn get_word(lexer: ^Lexer): str {
    let start := lexer.current;

    for 
        let c := 0;
        lexer.current < len(lexer.source.string) and !is_delim(substr(lexer.source.string, lexer.current, 1u64));
        next_char(lexer); 
    {}

    let word := substr(lexer.source.string, start, lexer.current - start);
    lexer.current = lexer.current - 1u64; # go back to delimeter
    return word;
}

fn is_delim(c: str): bool {
    return c == " " or c == "\n" or c == ";" or c == "(" or c == ")" or c == "{" or c == "}" or c == "," or c == ":" or
           c == "=" or c == "+" or c == "^" or c == "@" or c == "*" or c == "." or c == "[" or c == "]" or c == "!" or
           c == "<" or c == ">" or c == "|" or c == "-" or c == "/" or c == "%";
}

test {
    fn lex_test(): void {
        set_allocator("malloc", 0u64);
        
        let lexer := make_lexer("/Users/jackdelahunt/Projects/Liam/liamc/selfhost/tests/lex_test1.liam");
        lex(@lexer);
        
        assert_true(lexer.tokens.length == 7u64);
        
        let fn_token := array_index[Token](@lexer.tokens, 0u64);
        assert_true(fn_token.type == TokenType.FN);
        let x := fn_token.hello;
    }
}